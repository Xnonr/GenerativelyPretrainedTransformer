{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe701aee",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (from torch) (4.2.0)\n",
      "Requirement already satisfied: torchvision in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (0.14.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (from torchvision) (9.1.1)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (from torchvision) (4.2.0)\n",
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (from torchvision) (1.23.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (from requests->torchvision) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (from requests->torchvision) (2.0.12)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-0.13.1-cp310-cp310-macosx_12_0_arm64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (from torchaudio) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (from torch->torchaudio) (4.2.0)\n",
      "Installing collected packages: torchaudio\n",
      "Successfully installed torchaudio-0.13.1\n"
     ]
    }
   ],
   "source": [
    "# Installing dependencies\n",
    "#!pip3 install --upgrade pip\n",
    "#!pip3 install wget\n",
    "#!pip3 install torch \n",
    "#!pip3 install torchvision \n",
    "#!pip3 install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "51a86dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (1.11.0)\n",
      "Collecting torch\n",
      "  Downloading torch-1.13.1-cp310-none-macosx_11_0_arm64.whl (53.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (0.14.1)\n",
      "Requirement already satisfied: torchaudio in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (0.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (from torch) (4.2.0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (from torchvision) (9.1.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (from torchvision) (1.23.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (from requests->torchvision) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages (from requests->torchvision) (2022.6.15)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.11.0\n",
      "    Uninstalling torch-1.11.0:\n",
      "      Successfully uninstalled torch-1.11.0\n",
      "Successfully installed torch-1.13.1\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "!pip3 install --upgrade torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "619491d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "# Importing dependencies\n",
    "import torch\n",
    "#from torch.backends import mps\n",
    "print(torch.__version__)\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import wget\n",
    "\n",
    "from os.path import exists\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b24d35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33dc9f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters & Constants\n",
    "batch_size = 64\n",
    "block_size = 256\n",
    "\n",
    "# Number of iterations boosted to compensate for lower learning rates\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "\n",
    "# Self attention cannot tolerate very high learning rates well\n",
    "learning_rate = 3e-4\n",
    "\n",
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# For Mac M1 Max\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "# Dropout takes a neural network and every forward and backwards pass across it shuts off subsets of nodes, neurons\n",
    "# tokens, setting them to 0 and training without them, resulting in a training of an ensemble of networks, \n",
    "# 'merging' them at test time when all nodes are taken back online\n",
    "# Used as a regularization technique when scaling up the model avoid overfitting\n",
    "# Every forward and backward pass 20% intermediate calculations are disabled and dropped to 0\n",
    "dropout = 0.2\n",
    "\n",
    "eval_iters = 200\n",
    "number_of_embedding_dimensions = 384\n",
    "\n",
    "number_of_heads = 6\n",
    "number_of_layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b257721f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecd2a099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0% [                                                        ]       0 / 1115394\r",
      "  0% [                                                        ]    8192 / 1115394\r",
      "  1% [                                                        ]   16384 / 1115394\r",
      "  2% [.                                                       ]   24576 / 1115394\r",
      "  2% [.                                                       ]   32768 / 1115394\r",
      "  3% [..                                                      ]   40960 / 1115394\r",
      "  4% [..                                                      ]   49152 / 1115394\r",
      "  5% [..                                                      ]   57344 / 1115394\r",
      "  5% [...                                                     ]   65536 / 1115394\r",
      "  6% [...                                                     ]   73728 / 1115394\r",
      "  7% [....                                                    ]   81920 / 1115394\r",
      "  8% [....                                                    ]   90112 / 1115394\r",
      "  8% [....                                                    ]   98304 / 1115394\r",
      "  9% [.....                                                   ]  106496 / 1115394\r",
      " 10% [.....                                                   ]  114688 / 1115394\r",
      " 11% [......                                                  ]  122880 / 1115394\r",
      " 11% [......                                                  ]  131072 / 1115394\r",
      " 12% [......                                                  ]  139264 / 1115394\r",
      " 13% [.......                                                 ]  147456 / 1115394\r",
      " 13% [.......                                                 ]  155648 / 1115394\r",
      " 14% [........                                                ]  163840 / 1115394\r",
      " 15% [........                                                ]  172032 / 1115394\r",
      " 16% [.........                                               ]  180224 / 1115394\r",
      " 16% [.........                                               ]  188416 / 1115394\r",
      " 17% [.........                                               ]  196608 / 1115394\r",
      " 18% [..........                                              ]  204800 / 1115394\r",
      " 19% [..........                                              ]  212992 / 1115394\r",
      " 19% [...........                                             ]  221184 / 1115394\r",
      " 20% [...........                                             ]  229376 / 1115394\r",
      " 21% [...........                                             ]  237568 / 1115394\r",
      " 22% [............                                            ]  245760 / 1115394\r",
      " 22% [............                                            ]  253952 / 1115394\r",
      " 23% [.............                                           ]  262144 / 1115394\r",
      " 24% [.............                                           ]  270336 / 1115394\r",
      " 24% [.............                                           ]  278528 / 1115394\r",
      " 25% [..............                                          ]  286720 / 1115394\r",
      " 26% [..............                                          ]  294912 / 1115394\r",
      " 27% [...............                                         ]  303104 / 1115394\r",
      " 27% [...............                                         ]  311296 / 1115394\r",
      " 28% [................                                        ]  319488 / 1115394\r",
      " 29% [................                                        ]  327680 / 1115394\r",
      " 30% [................                                        ]  335872 / 1115394\r",
      " 30% [.................                                       ]  344064 / 1115394\r",
      " 31% [.................                                       ]  352256 / 1115394\r",
      " 32% [..................                                      ]  360448 / 1115394\r",
      " 33% [..................                                      ]  368640 / 1115394\r",
      " 33% [..................                                      ]  376832 / 1115394\r",
      " 34% [...................                                     ]  385024 / 1115394\r",
      " 35% [...................                                     ]  393216 / 1115394\r",
      " 35% [....................                                    ]  401408 / 1115394\r",
      " 36% [....................                                    ]  409600 / 1115394\r",
      " 37% [....................                                    ]  417792 / 1115394\r",
      " 38% [.....................                                   ]  425984 / 1115394\r",
      " 38% [.....................                                   ]  434176 / 1115394\r",
      " 39% [......................                                  ]  442368 / 1115394\r",
      " 40% [......................                                  ]  450560 / 1115394\r",
      " 41% [.......................                                 ]  458752 / 1115394\r",
      " 41% [.......................                                 ]  466944 / 1115394\r",
      " 42% [.......................                                 ]  475136 / 1115394\r",
      " 43% [........................                                ]  483328 / 1115394\r",
      " 44% [........................                                ]  491520 / 1115394\r",
      " 44% [.........................                               ]  499712 / 1115394\r",
      " 45% [.........................                               ]  507904 / 1115394\r",
      " 46% [.........................                               ]  516096 / 1115394\r",
      " 47% [..........................                              ]  524288 / 1115394\r",
      " 47% [..........................                              ]  532480 / 1115394\r",
      " 48% [...........................                             ]  540672 / 1115394\r",
      " 49% [...........................                             ]  548864 / 1115394\r",
      " 49% [...........................                             ]  557056 / 1115394\r",
      " 50% [............................                            ]  565248 / 1115394\r",
      " 51% [............................                            ]  573440 / 1115394\r",
      " 52% [.............................                           ]  581632 / 1115394\r",
      " 52% [.............................                           ]  589824 / 1115394\r",
      " 53% [..............................                          ]  598016 / 1115394\r",
      " 54% [..............................                          ]  606208 / 1115394\r",
      " 55% [..............................                          ]  614400 / 1115394\r",
      " 55% [...............................                         ]  622592 / 1115394\r",
      " 56% [...............................                         ]  630784 / 1115394\r",
      " 57% [................................                        ]  638976 / 1115394\r",
      " 58% [................................                        ]  647168 / 1115394\r",
      " 58% [................................                        ]  655360 / 1115394\r",
      " 59% [.................................                       ]  663552 / 1115394\r",
      " 60% [.................................                       ]  671744 / 1115394\r",
      " 60% [..................................                      ]  679936 / 1115394\r",
      " 61% [..................................                      ]  688128 / 1115394\r",
      " 62% [..................................                      ]  696320 / 1115394\r",
      " 63% [...................................                     ]  704512 / 1115394\r",
      " 63% [...................................                     ]  712704 / 1115394\r",
      " 64% [....................................                    ]  720896 / 1115394\r",
      " 65% [....................................                    ]  729088 / 1115394\r",
      " 66% [.....................................                   ]  737280 / 1115394\r",
      " 66% [.....................................                   ]  745472 / 1115394\r",
      " 67% [.....................................                   ]  753664 / 1115394\r",
      " 68% [......................................                  ]  761856 / 1115394\r",
      " 69% [......................................                  ]  770048 / 1115394\r",
      " 69% [.......................................                 ]  778240 / 1115394\r",
      " 70% [.......................................                 ]  786432 / 1115394\r",
      " 71% [.......................................                 ]  794624 / 1115394\r",
      " 71% [........................................                ]  802816 / 1115394\r",
      " 72% [........................................                ]  811008 / 1115394\r",
      " 73% [.........................................               ]  819200 / 1115394\r",
      " 74% [.........................................               ]  827392 / 1115394\r",
      " 74% [.........................................               ]  835584 / 1115394\r",
      " 75% [..........................................              ]  843776 / 1115394\r",
      " 76% [..........................................              ]  851968 / 1115394\r",
      " 77% [...........................................             ]  860160 / 1115394\r",
      " 77% [...........................................             ]  868352 / 1115394\r",
      " 78% [............................................            ]  876544 / 1115394\r",
      " 79% [............................................            ]  884736 / 1115394\r",
      " 80% [............................................            ]  892928 / 1115394\r",
      " 80% [.............................................           ]  901120 / 1115394\r",
      " 81% [.............................................           ]  909312 / 1115394\r",
      " 82% [..............................................          ]  917504 / 1115394\r",
      " 82% [..............................................          ]  925696 / 1115394\r",
      " 83% [..............................................          ]  933888 / 1115394\r",
      " 84% [...............................................         ]  942080 / 1115394\r",
      " 85% [...............................................         ]  950272 / 1115394\r",
      " 85% [................................................        ]  958464 / 1115394\r",
      " 86% [................................................        ]  966656 / 1115394\r",
      " 87% [................................................        ]  974848 / 1115394\r",
      " 88% [.................................................       ]  983040 / 1115394\r",
      " 88% [.................................................       ]  991232 / 1115394\r",
      " 89% [..................................................      ]  999424 / 1115394\r",
      " 90% [..................................................      ] 1007616 / 1115394\r",
      " 91% [...................................................     ] 1015808 / 1115394\r",
      " 91% [...................................................     ] 1024000 / 1115394\r",
      " 92% [...................................................     ] 1032192 / 1115394\r",
      " 93% [....................................................    ] 1040384 / 1115394\r",
      " 94% [....................................................    ] 1048576 / 1115394\r",
      " 94% [.....................................................   ] 1056768 / 1115394\r",
      " 95% [.....................................................   ] 1064960 / 1115394\r",
      " 96% [.....................................................   ] 1073152 / 1115394\r",
      " 96% [......................................................  ] 1081344 / 1115394\r",
      " 97% [......................................................  ] 1089536 / 1115394\r",
      " 98% [....................................................... ] 1097728 / 1115394\r",
      " 99% [....................................................... ] 1105920 / 1115394\r",
      " 99% [....................................................... ] 1114112 / 1115394\r",
      "100% [........................................................] 1115394 / 1115394"
     ]
    }
   ],
   "source": [
    "# Downloading the Tiny Shakespeare dataset\n",
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "file_name = wget.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e17b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataset to a text file\n",
    "#with open('tiny_shakespeare.txt', 'wb') as file:\n",
    "#    file.write(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "14dd7fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n"
     ]
    }
   ],
   "source": [
    "if exists('input.txt') == True:\n",
    "    print('Hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51989dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and inspecting the downloaded dataset\n",
    "with open('input.txt', 'r', encoding = 'utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b2f659f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset via character count:  1115394\n"
     ]
    }
   ],
   "source": [
    "print('Length of dataset via character count: ', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a159447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Observing the first 1000 characters of the text dataset\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "265fb149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz \n",
      "\n",
      "Vocabulary Size: 65\n"
     ]
    }
   ],
   "source": [
    "# Uncovering all of the unique characters which appear throughout the dataset\n",
    "unique_characters = sorted(list(set(text)))\n",
    "vocabulary_size = len(unique_characters)\n",
    "\n",
    "print(f'Unique Characters: {\"\".join(unique_characters)} \\n')\n",
    "print(f'Vocabulary Size: {vocabulary_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9d3663fa",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n', 0) | (0, '\\n')\n",
      "(' ', 1) | (1, ' ')\n",
      "('!', 2) | (2, '!')\n",
      "('$', 3) | (3, '$')\n",
      "('&', 4) | (4, '&')\n",
      "(\"'\", 5) | (5, \"'\")\n",
      "(',', 6) | (6, ',')\n",
      "('-', 7) | (7, '-')\n",
      "('.', 8) | (8, '.')\n",
      "('3', 9) | (9, '3')\n",
      "(':', 10) | (10, ':')\n",
      "(';', 11) | (11, ';')\n",
      "('?', 12) | (12, '?')\n",
      "('A', 13) | (13, 'A')\n",
      "('B', 14) | (14, 'B')\n",
      "('C', 15) | (15, 'C')\n",
      "('D', 16) | (16, 'D')\n",
      "('E', 17) | (17, 'E')\n",
      "('F', 18) | (18, 'F')\n",
      "('G', 19) | (19, 'G')\n",
      "('H', 20) | (20, 'H')\n",
      "('I', 21) | (21, 'I')\n",
      "('J', 22) | (22, 'J')\n",
      "('K', 23) | (23, 'K')\n",
      "('L', 24) | (24, 'L')\n",
      "('M', 25) | (25, 'M')\n",
      "('N', 26) | (26, 'N')\n",
      "('O', 27) | (27, 'O')\n",
      "('P', 28) | (28, 'P')\n",
      "('Q', 29) | (29, 'Q')\n",
      "('R', 30) | (30, 'R')\n",
      "('S', 31) | (31, 'S')\n",
      "('T', 32) | (32, 'T')\n",
      "('U', 33) | (33, 'U')\n",
      "('V', 34) | (34, 'V')\n",
      "('W', 35) | (35, 'W')\n",
      "('X', 36) | (36, 'X')\n",
      "('Y', 37) | (37, 'Y')\n",
      "('Z', 38) | (38, 'Z')\n",
      "('a', 39) | (39, 'a')\n",
      "('b', 40) | (40, 'b')\n",
      "('c', 41) | (41, 'c')\n",
      "('d', 42) | (42, 'd')\n",
      "('e', 43) | (43, 'e')\n",
      "('f', 44) | (44, 'f')\n",
      "('g', 45) | (45, 'g')\n",
      "('h', 46) | (46, 'h')\n",
      "('i', 47) | (47, 'i')\n",
      "('j', 48) | (48, 'j')\n",
      "('k', 49) | (49, 'k')\n",
      "('l', 50) | (50, 'l')\n",
      "('m', 51) | (51, 'm')\n",
      "('n', 52) | (52, 'n')\n",
      "('o', 53) | (53, 'o')\n",
      "('p', 54) | (54, 'p')\n",
      "('q', 55) | (55, 'q')\n",
      "('r', 56) | (56, 'r')\n",
      "('s', 57) | (57, 's')\n",
      "('t', 58) | (58, 't')\n",
      "('u', 59) | (59, 'u')\n",
      "('v', 60) | (60, 'v')\n",
      "('w', 61) | (61, 'w')\n",
      "('x', 62) | (62, 'x')\n",
      "('y', 63) | (63, 'y')\n",
      "('z', 64) | (64, 'z')\n"
     ]
    }
   ],
   "source": [
    "# Establishing an encoder, decoder tokenizer unique to this dataset by\n",
    "# creating a mapping from characters to integers in the form of a dictionary\n",
    "\n",
    "string_to_integer = {character:integer for integer, character in enumerate(unique_characters)}\n",
    "integer_to_string = {integer:character for integer, character in enumerate(unique_characters)}\n",
    "\n",
    "#print(type(string_to_integer))\n",
    "#print(type(integer_to_string))\n",
    "\n",
    "#print(f'{string_to_integer} \\n')\n",
    "#print(f'{integer_to_string} \\n')\n",
    "\n",
    "for dict1_key_value_pair, dict2_key_value_pair in zip(string_to_integer.items(), integer_to_string.items()):\n",
    "    print(f'{dict1_key_value_pair} | {dict2_key_value_pair}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e6d63819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder taking in a string and outputing a list of integers\n",
    "encoder = lambda string: [string_to_integer[character] for character in string]\n",
    "\n",
    "# Decoder taking in a list of integers and outputing a string\n",
    "decoder1 = lambda list_of_integers: ''.join([integer_to_string[integer] for integer in list_of_integers])\n",
    "\n",
    "def decoder2(encoded_string):\n",
    "    decoded_string = ''\n",
    "    \n",
    "    for integer in encoded_string:\n",
    "        #print(f'Integer to decode: {integer}')\n",
    "        \n",
    "        character = integer_to_string[integer]\n",
    "        \n",
    "        #print(f'Decoded integer: {character}')\n",
    "        \n",
    "        decoded_string += character\n",
    "    \n",
    "    return decoded_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "adfb4b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test String: Hello There!\n",
      "Encoded Test String: [20, 43, 50, 50, 53, 1, 32, 46, 43, 56, 43, 2]\n",
      "Decoded Test String: Hello There!\n"
     ]
    }
   ],
   "source": [
    "# There is a tradeoff between a tokenizer's overall total codebook size and the associated lengths of said\n",
    "# codebook's sequences of integers\n",
    "\n",
    "# Either very long sequences of integers with very small vocabularies\n",
    "# or very short sequences of integers with very large vocabularies\n",
    "\n",
    "# Typically most tokenizer dictionaries used are somewhere in the middle, making use of subwords\n",
    "\n",
    "# As a character level tokenizer is being utilized, the sequences of integers lengths are very long, \n",
    "# but the size of the overall codebook is very small, as there are only 64 unique characters within \n",
    "# the Tiny Shakespeare dataset\n",
    "\n",
    "test_string = 'Hello There!'\n",
    "encoded_test_string = encoder(test_string)\n",
    "decoded_test_string = decoder1(encoded_test_string)\n",
    "\n",
    "print(f'Test String: {test_string}')\n",
    "print(f'Encoded Test String: {encoded_test_string}')\n",
    "print(f'Decoded Test String: {decoded_test_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "010b4a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the entire text dataset whilst storing it within a PyTorch tensor\n",
    "data = torch.tensor(encoder(text), dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "26f2d417",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([1115394]) | DType: torch.int64 \n",
      "\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# Some data about the tensor\n",
    "print(f'Shape: {data.shape} | DType: {data.dtype} \\n')\n",
    "\n",
    "# Observing the first 1000 characters of the now encoded and tokenized dataset\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7b0c976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into the respective training and validation datasets\n",
    "index = int(0.9 * len(data))\n",
    "\n",
    "# Vast majority of the dataset for this example is used exclusively for training purposes\n",
    "training_data = data[:index]\n",
    "\n",
    "# Utilized to observe as to whether the model is overfitting, i.e. simply memorizing actual passages of\n",
    "# Shakespeare rather than generating something which looks like Shakespeare\n",
    "validation_data = data[index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5a7a3a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In real life scenarios and use cases, it is often prohibitively expensive or practically impossible to\n",
    "# pass all of the relevant data into a transformer at once, and so it must be chunked into portions\n",
    "# that said transformer will randomly select from throughout the available training data\n",
    "# limited to a certain size or length defined by what is called block size or context length, for example\n",
    "block_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "92e48b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A chunk contains an equivalent of the block_size - 1 number of examples for the transformer to train upon\n",
    "\n",
    "# For example, if a block contains the phrase 'Hello, my name is John.', a transformer using words as its tokens\n",
    "# would see the example that the word 'Hello' is followed by the word 'my', and that the subphrase 'Hello my' is\n",
    "# then followed by the word 'is', and so on and so forth\n",
    "\n",
    "# Think of this as a dimension of time, what comes before and proceeds after something, giving context\n",
    "\n",
    "# With this dataset, it is the individual characters which follow or proceed one another that serve as the basis\n",
    "# for this process of learning by the transformer\n",
    "training_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4ae8b3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the input is: tensor([18])\n",
      "The target is: 47\n",
      "\n",
      "When the input is: tensor([18, 47])\n",
      "The target is: 56\n",
      "\n",
      "When the input is: tensor([18, 47, 56])\n",
      "The target is: 57\n",
      "\n",
      "When the input is: tensor([18, 47, 56, 57])\n",
      "The target is: 58\n",
      "\n",
      "When the input is: tensor([18, 47, 56, 57, 58])\n",
      "The target is: 1\n",
      "\n",
      "When the input is: tensor([18, 47, 56, 57, 58,  1])\n",
      "The target is: 15\n",
      "\n",
      "When the input is: tensor([18, 47, 56, 57, 58,  1, 15])\n",
      "The target is: 47\n",
      "\n",
      "When the input is: tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "The target is: 58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For a deeper understanding, see below:\n",
    "x = training_data[:block_size]\n",
    "y = training_data[1:block_size + 1] # Index offset of 1 required to properly link a context to its target\n",
    "\n",
    "for z in range(block_size):\n",
    "    \n",
    "    # The context is always all of the characters, words, etc. up to z\n",
    "    context = x[:z + 1]\n",
    "    \n",
    "    # The target is always z\n",
    "    target = y[z]\n",
    "    \n",
    "    print(f'When the input is: {context}\\nThe target is: {target}\\n')\n",
    "    \n",
    "# Transformers are trained in this fashion not just from an efficiency standpoint, as all of the data within\n",
    "# the given chunk is already available at the given moment, but also as it gets the transformer used to seeing\n",
    "# examples of various length and context from a length of 1 all the way up to the maximum given block size\n",
    "\n",
    "# Once a transformer's predictions reach blocksize, the prediction is truncated as said transformer will never\n",
    "# have been trained based upon a context greater than its given blocksize, inputs greater than the alloted\n",
    "# blocksize will also be truncated or chunked appropriately to fit this restriction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2be1537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch dimension is also a consideration for transformers\n",
    "\n",
    "# Many batches of multiple chunks of text will be fed all at once into a transformer, all wrapped up within a \n",
    "# single tensor for the purposes of efficiency, as GPUs are designed to deal with these sorts of inputs \n",
    "# processing them in parallel during the transformer's training\n",
    "\n",
    "# These chunks are processed completly independently and never communicate with one another during this process\n",
    "\n",
    "# Seed set for reproducibility of results due to the random sampling of chunks from throughout the training data\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# The number of independent sequences which will be processing in parallel\n",
    "#batch_size = 4\n",
    "\n",
    "# The maximum content length for predictions\n",
    "#block_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2fc7f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    \n",
    "    # Generates a small batch of data of 'x' inputs and 'y' targets\n",
    "    \n",
    "    # If the split is a training split, it will look through the training data, otherwise it will look into\n",
    "    # the validation data\n",
    "    data = training_data if split == 'training' else validation_data\n",
    "    \n",
    "    # Generates a tensor of random offsets within the chosen dataset based off of the desired batch size\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    \n",
    "    # The first block size characters starting at an index of 'i'\n",
    "    # Each of these 'x' chunks are retrieved for every randomized index starting point of 'i' taken from 'ix'\n",
    "    # These chunks of 1 dimensional tensors are then stacked as rows within a tensor table of dimensions\n",
    "    # batch size * block size\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    \n",
    "    # Same as x but offset by 1, the target from the context given by x\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    \n",
    "    # Ensuring that if a GPU is being ultilized or is available, that the data is moved over to that\n",
    "    # device before progressing on\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "39db821b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      "Batch Shape: torch.Size([32, 8])\n",
      "tensor([[56, 43, 58, 53, 50, 42,  1, 58],\n",
      "        [39, 49, 43,  1, 43, 45, 45, 57],\n",
      "        [ 1, 52, 53,  1, 40, 43, 39, 57],\n",
      "        [51,  1, 46, 39, 60, 43,  1, 41],\n",
      "        [ 1, 46, 39, 60, 43,  1, 40, 56],\n",
      "        [ 1, 53, 44, 44,  8,  0, 21, 58],\n",
      "        [ 6,  1, 58, 46, 43,  1, 46, 43],\n",
      "        [50, 43, 57, 57,  1, 54, 53, 61],\n",
      "        [58, 46, 43,  1, 40, 50, 39, 51],\n",
      "        [26, 43, 39, 54, 53, 50, 47, 58],\n",
      "        [59, 57, 58,  1, 47, 58,  1, 40],\n",
      "        [10,  0, 21,  1, 42, 39, 56, 43],\n",
      "        [ 6,  1, 47, 44,  1, 58, 46, 63],\n",
      "        [56, 11,  1, 46, 43, 56, 43,  1],\n",
      "        [10,  0, 15, 39, 52,  1, 57, 47],\n",
      "        [42,  1, 47, 58,  1, 52, 53, 58],\n",
      "        [39, 58, 46,  1, 52, 53, 58,  1],\n",
      "        [46, 47, 57,  1, 40, 43, 50, 50],\n",
      "        [43, 39, 56,  1, 51, 43,  6,  1],\n",
      "        [42, 10,  1, 40, 59, 58,  1, 46],\n",
      "        [53, 59,  1, 44, 50, 39, 58, 58],\n",
      "        [52,  6,  0, 35, 43, 56, 43,  1],\n",
      "        [59, 58,  1, 51, 43, 43, 49, 52],\n",
      "        [47, 60, 43,  6,  1, 39, 52, 42],\n",
      "        [43, 50, 44,  6,  0, 32, 46, 53],\n",
      "        [ 1, 39,  1, 52, 53, 58, 43,  1],\n",
      "        [33, 24, 21, 17, 32, 10,  0, 21],\n",
      "        [51, 59, 56, 42, 43, 56, 10,  1],\n",
      "        [ 1, 58, 46, 43,  1, 46, 39, 57],\n",
      "        [53, 59,  1, 44, 39, 50, 50,  1],\n",
      "        [21, 33, 31, 10,  0, 13, 50, 50],\n",
      "        [ 1, 46, 39, 60, 43,  1, 58, 46]])\n",
      "\n",
      "\n",
      "Targets: \n",
      "Batch Shape: torch.Size([32, 8])\n",
      "tensor([[43, 58, 53, 50, 42,  1, 58, 46],\n",
      "        [49, 43,  1, 43, 45, 45, 57,  1],\n",
      "        [52, 53,  1, 40, 43, 39, 57, 58],\n",
      "        [ 1, 46, 39, 60, 43,  1, 41, 59],\n",
      "        [46, 39, 60, 43,  1, 40, 56, 53],\n",
      "        [53, 44, 44,  8,  0, 21, 58,  1],\n",
      "        [ 1, 58, 46, 43,  1, 46, 43, 39],\n",
      "        [43, 57, 57,  1, 54, 53, 61, 43],\n",
      "        [46, 43,  1, 40, 50, 39, 51, 43],\n",
      "        [43, 39, 54, 53, 50, 47, 58, 39],\n",
      "        [57, 58,  1, 47, 58,  1, 40, 43],\n",
      "        [ 0, 21,  1, 42, 39, 56, 43,  1],\n",
      "        [ 1, 47, 44,  1, 58, 46, 63,  1],\n",
      "        [11,  1, 46, 43, 56, 43,  1, 41],\n",
      "        [ 0, 15, 39, 52,  1, 57, 47, 41],\n",
      "        [ 1, 47, 58,  1, 52, 53, 58,  1],\n",
      "        [58, 46,  1, 52, 53, 58,  1, 51],\n",
      "        [47, 57,  1, 40, 43, 50, 50, 57],\n",
      "        [39, 56,  1, 51, 43,  6,  1, 39],\n",
      "        [10,  1, 40, 59, 58,  1, 46, 43],\n",
      "        [59,  1, 44, 50, 39, 58, 58, 43],\n",
      "        [ 6,  0, 35, 43, 56, 43,  1, 40],\n",
      "        [58,  1, 51, 43, 43, 49, 52, 43],\n",
      "        [60, 43,  6,  1, 39, 52, 42,  1],\n",
      "        [50, 44,  6,  0, 32, 46, 53, 59],\n",
      "        [39,  1, 52, 53, 58, 43,  1, 44],\n",
      "        [24, 21, 17, 32, 10,  0, 21, 52],\n",
      "        [59, 56, 42, 43, 56, 10,  1, 39],\n",
      "        [58, 46, 43,  1, 46, 39, 57, 58],\n",
      "        [59,  1, 44, 39, 50, 50,  1, 47],\n",
      "        [33, 31, 10,  0, 13, 50, 50,  5],\n",
      "        [46, 39, 60, 43,  1, 58, 46, 43]])\n",
      "\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# This example spells out how there will be 32 independent examples of input 'x' which will be fed into a\n",
    "# transformer, which will reference the input 'y' as a basis for its future predictions\n",
    "xb, yb = get_batch('training')\n",
    "\n",
    "print('Inputs: ')\n",
    "print(f'Batch Shape: {xb.shape}')\n",
    "print(xb)\n",
    "print('\\n')\n",
    "\n",
    "print('Targets: ')\n",
    "print(f'Batch Shape: {yb.shape}')\n",
    "print(yb)\n",
    "print('\\n')\n",
    "\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6fc30213",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the input is: tensor([24])\n",
      "The target is: 43\n",
      "\n",
      "When the input is: tensor([24, 43])\n",
      "The target is: 58\n",
      "\n",
      "When the input is: tensor([24, 43, 58])\n",
      "The target is: 5\n",
      "\n",
      "When the input is: tensor([24, 43, 58,  5])\n",
      "The target is: 57\n",
      "\n",
      "When the input is: tensor([24, 43, 58,  5, 57])\n",
      "The target is: 1\n",
      "\n",
      "When the input is: tensor([24, 43, 58,  5, 57,  1])\n",
      "The target is: 46\n",
      "\n",
      "When the input is: tensor([24, 43, 58,  5, 57,  1, 46])\n",
      "The target is: 43\n",
      "\n",
      "When the input is: tensor([24, 43, 58,  5, 57,  1, 46, 43])\n",
      "The target is: 39\n",
      "\n",
      "When the input is: tensor([44])\n",
      "The target is: 53\n",
      "\n",
      "When the input is: tensor([44, 53])\n",
      "The target is: 56\n",
      "\n",
      "When the input is: tensor([44, 53, 56])\n",
      "The target is: 1\n",
      "\n",
      "When the input is: tensor([44, 53, 56,  1])\n",
      "The target is: 58\n",
      "\n",
      "When the input is: tensor([44, 53, 56,  1, 58])\n",
      "The target is: 46\n",
      "\n",
      "When the input is: tensor([44, 53, 56,  1, 58, 46])\n",
      "The target is: 39\n",
      "\n",
      "When the input is: tensor([44, 53, 56,  1, 58, 46, 39])\n",
      "The target is: 58\n",
      "\n",
      "When the input is: tensor([44, 53, 56,  1, 58, 46, 39, 58])\n",
      "The target is: 1\n",
      "\n",
      "When the input is: tensor([52])\n",
      "The target is: 58\n",
      "\n",
      "When the input is: tensor([52, 58])\n",
      "The target is: 1\n",
      "\n",
      "When the input is: tensor([52, 58,  1])\n",
      "The target is: 58\n",
      "\n",
      "When the input is: tensor([52, 58,  1, 58])\n",
      "The target is: 46\n",
      "\n",
      "When the input is: tensor([52, 58,  1, 58, 46])\n",
      "The target is: 39\n",
      "\n",
      "When the input is: tensor([52, 58,  1, 58, 46, 39])\n",
      "The target is: 58\n",
      "\n",
      "When the input is: tensor([52, 58,  1, 58, 46, 39, 58])\n",
      "The target is: 1\n",
      "\n",
      "When the input is: tensor([52, 58,  1, 58, 46, 39, 58,  1])\n",
      "The target is: 46\n",
      "\n",
      "When the input is: tensor([25])\n",
      "The target is: 17\n",
      "\n",
      "When the input is: tensor([25, 17])\n",
      "The target is: 27\n",
      "\n",
      "When the input is: tensor([25, 17, 27])\n",
      "The target is: 10\n",
      "\n",
      "When the input is: tensor([25, 17, 27, 10])\n",
      "The target is: 0\n",
      "\n",
      "When the input is: tensor([25, 17, 27, 10,  0])\n",
      "The target is: 21\n",
      "\n",
      "When the input is: tensor([25, 17, 27, 10,  0, 21])\n",
      "The target is: 1\n",
      "\n",
      "When the input is: tensor([25, 17, 27, 10,  0, 21,  1])\n",
      "The target is: 54\n",
      "\n",
      "When the input is: tensor([25, 17, 27, 10,  0, 21,  1, 54])\n",
      "The target is: 39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spelling it out\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        \n",
    "        context = xb[b, :t + 1]\n",
    "        target = yb[b, t]\n",
    "        \n",
    "        print(f'When the input is: {context}\\nThe target is: {target}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "06bdcc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "# Example input into a transformer\n",
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2a0e6623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram Language Model\n",
    "# One of the simplest neural network language models\n",
    "\n",
    "# Custom subclass of the 'nn' module\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    # Technically no need to pass in the 'vocabulary_size' variable into the constructor at it already\n",
    "    # exists above as a global variable, kept in as a personal preference\n",
    "    def __init__(self, vocabulary_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Creating a token embedding table\n",
    "        # There is a very thin wrapper around a tensor of shape vocabulary size * vocabulary size\n",
    "        # Each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocabulary_size, vocabulary_size)\n",
    "        \n",
    "    def forward(self, idx, targets = None):\n",
    "        \n",
    "        # When an index is passed in, every single integer from the input will refer to the embedding\n",
    "        # table and will pluck out a row of said table coresponding to its index\n",
    "        # An example would be an object tokenized as the integer number 24 would select the row with index\n",
    "        # of 24 from the embedding table\n",
    "        \n",
    "        # 'idx' and 'targets' are both (Batch, Time) tensor of integers\n",
    "        \n",
    "        # (Batch, Time, Channel) Tensor will be arranged by PyTorch\n",
    "        # (4, 8, 65 the vocabulary_size)\n",
    "        # Logits are the scores for the next characters in the sequence\n",
    "        # A prediction for what comes next is based off of the single individual identity of a token\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        \n",
    "        # Loss is the crossentropy of the predictions upon the targets\n",
    "        # Measures the quality of the logits with respect to the targets\n",
    "        # We have the actual identity of the next character, how well is the next character being predicted \n",
    "        # based upon the logits\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        \n",
    "        else:\n",
    "            # For loss calculations to run, PyTorch requires an input of form (Batch, Channel, Time) \n",
    "            # for multidimensional inputs\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # Reducing a 3D array to a 2D array, stretching out the 2D 'xb' input into a 1D column, \n",
    "            # preserving the Channel as the second 'column' to better conform to Pytorch's requirements\n",
    "            logits = logits.view(B * T, C)\n",
    "\n",
    "            # Targets must also be reshaped as a result\n",
    "            targets = targets.view(B * T) # Explicit\n",
    "            #targets = targets.view(-1) # Otherwise PyTorch will attempt to guess the best shape out the output\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    # The purpose of this function is to continue the generation in all of the batch dimensions in the time \n",
    "    # dimension of (B, T) with relation to 'idx', so that you end up with (B, T + 1), (B, T + 2), etc.\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \n",
    "        #idx is (B, T), an array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # Retrieving the predictions from the current index\n",
    "            logits, loss = self(idx)\n",
    "            \n",
    "            # Focusing solely upon the last time step\n",
    "            # Removing the last element of the time dimension as those are the predictions for what comes next\n",
    "            logits = logits[:, -1, :] # (B, T) becomes (B, C)\n",
    "            \n",
    "            # Applying softmax in order to obtain probabilities via conversion of the logits\n",
    "            probs = F.softmax(logits, dim = -1) # (B, C)\n",
    "            \n",
    "            # Sampling from the distribution of probabilities, retrieving only 1 sample from PyTorch\n",
    "            # For each of the batch dimensions, there will now only be a single prediction for what comes next\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # (B, 1)\n",
    "            \n",
    "            # Appending sampled index to the running sequence\n",
    "            \n",
    "            # Whatever is predicted from 'idx_next' is concatenated on top of the previous 'idx' along\n",
    "            # the first dimension, the Time dimension\n",
    "            \n",
    "            # Takeing the integers from the above line's sampling process, concatenating them on top of the\n",
    "            # running steam of integers\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) # (B, T + 1)\n",
    "            \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4adb82e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Linear projections being applied to all tokens and nodes\n",
    "        self.key = nn.Linear(number_of_embedding_dimensions, head_size, bias = False)\n",
    "        self.query = nn.Linear(number_of_embedding_dimensions, head_size, bias = False)\n",
    "        self.value = nn.Linear(number_of_embedding_dimensions, head_size, bias = False)\n",
    "        \n",
    "        # 'tril' is not a parameter of the model, it is a buffer via PyTorch naming conventions\n",
    "        # Lower triangular matrix\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch, time, channels = x.shape\n",
    "        \n",
    "        # Calculating token keys\n",
    "        keys = self.key(x) # Shape of (batch, time, channels)\n",
    "        \n",
    "        # Calculating token queries\n",
    "        queries = self.query(x) # Shape of (batch, time, channels)\n",
    "        \n",
    "        # Computing attention scores, i.e. affinities\n",
    "        # (batch, time, channels) @ (batch, channels, time) -> (batch, time, time)\n",
    "        # Calculating attention scores within the weights, normalizing them via scaled attention\n",
    "        weights = queries @ keys.transpose(-2, -1) * channels ** -0.5\n",
    "        \n",
    "        # Ensuring future tokens cannot communicate with past tokens\n",
    "        weights = weights.masked_fill(self.tril[:time, :time] == 0, float('-inf')) # Shape of (batch, time, time)\n",
    "        weights = F.softmax(weights, dim = -1) # Shape of (batch, time, time)\n",
    "        \n",
    "        # Can potentially dropout, preventing random communication from nodes to avoid overfitting\n",
    "        weights = self.dropout(weights)\n",
    "        \n",
    "        # Performing the weighted aggregation of the values\n",
    "        values = self.value(x) # Shape of (batch, time, channels)\n",
    "        \n",
    "        # (batch, time, time) @ (batch, time, channels) -> (batch, time, channels)\n",
    "        output = weights @ values\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "623cef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Head Attention\n",
    "# Multiple heads are working in parallel in order to concatenate the relevant results\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, number_of_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(number_of_heads)])\n",
    "        self.projection = nn.Linear(number_of_embedding_dimensions, number_of_embedding_dimensions)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Concatenating results over the channel dimension\n",
    "        \n",
    "        # Output of the self attention\n",
    "        output = torch.cat([head(x) for head in self.heads], dim = -1)\n",
    "        \n",
    "        # Application of the projection onto the self attention output\n",
    "        # Projection is a linear transformation of the outcome of the previous layer\n",
    "        # Can potentially dropout at the multi attention head stage\n",
    "        output = self.dropout(self.projection(output))\n",
    "        \n",
    "        # Projection will be fed back into the residual pathway\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fefb365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary Revised Bigram Language Model\n",
    "class BigramLanguageModel2(nn.Module):\n",
    "    def __init__(self, vocabulary_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Instead of going diectly towards the embedding phase, a level of interaction is desired\n",
    "        # between the logits\n",
    "        self.token_embedding_table = nn.Embedding(vocabulary_size, number_of_embedding_dimensions)\n",
    "        \n",
    "        # Encoding indeces based upon the position of their tokens on top of the tokens' identities\n",
    "        # Each position from 0 to block size - 1 will also get their own embedding vector\n",
    "        self.postion_embedding_table == nn.Embedding(block_size, number_of_embedding_dimensions)\n",
    "        \n",
    "        # Declaration of head size\n",
    "        self.self_attention_head = Head(number_of_embedding_dimensions)\n",
    "        \n",
    "        # Logits are no longer aquired directly, instead first token embeddings are retrieved\n",
    "        # In order to go from the token embeddings to the logits, a linear layer is required\n",
    "        self.language_modeling_head = nn.Linear(number_of_embedding_dimensions, vocabulary_size)\n",
    "        \n",
    "    def forward(self, index, targets = None):\n",
    "        batch, time = index.shape\n",
    "        \n",
    "        # Indeces are encoded based upon the identity of the tokens\n",
    "        \n",
    "        # 'index' and 'targets' are both (batch, time) shaped tensors of integers\n",
    "        # Logits are no longer aquired directly, instead first token embeddings are retrieved\n",
    "        \n",
    "        # Right aligned, a new dimension of 1 gets added, and then broadcasted across batch\n",
    "        token_embeddings = self.token_embedding_table(index) # Shape (batch, time, channels_1)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(time, device = device)) # (time, channels)\n",
    "        \n",
    "        # Holds the token identities as well as the positions at which said tokens occur\n",
    "        token_id_position_embeddings = token_embeddings + position_embeddings # (batch, time, channels)\n",
    "        \n",
    "        # Applying one head of self attention to the composite token embeddings\n",
    "        # Shape of (batch, time, channels)\n",
    "        token_id_position_embeddings = self.self_attention_head(token_id_position_embeddings)\n",
    "        \n",
    "        # Care must be taken, as the channels dimensions are no longer equal, as \n",
    "        # channels_1 = number_of_embedding_dimensions and channels_2 = vocabulary size\n",
    "        logits = self.language_modeling_head(token_id_position_embeddings) # Shape (batch, time, channels_2)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        \n",
    "        else:\n",
    "            batch, time, channels = logits.shape\n",
    "            logits = logits.view(batch * time, channels)\n",
    "            targets = targets.view(batch * time)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # As positional embedding is now in usage, never more than the set block size is allowed to\n",
    "            # come in at any one time, as since if an index is larger than the block size, an out of scope\n",
    "            # error will arise due to the position embedding table only going up to said block size\n",
    "            index_condition = index[:, -block_size:]\n",
    "            \n",
    "            logits, loss = self(index_condition)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim = -1)\n",
    "            index_next = torch.multinomial(probs, num_samples = 1)\n",
    "            index = torch.cat((index, index_next), dim = 1)\n",
    "            \n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f395416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward\n",
    "# Essentially just a multi layer perceptron\n",
    "# Adding computation into the neural network at a per node level\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, number_of_embedding_dimensions):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Tokens original had time to look at one another but not enough time to truly 'think on'\n",
    "        # and contextualize said information, which is why these layers are added\n",
    "        # Occurs on a per token level, evey token and node doing this independently\n",
    "        self.net = nn.Sequential(\n",
    "            # The inner layer of the feed forward network should be 4 times as large as the output in terms\n",
    "            # of channel sizes\n",
    "            nn.Linear(number_of_embedding_dimensions, 4 * number_of_embedding_dimensions),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Projection layer going back into the residual pathway, shrinked back down to the proper size\n",
    "            nn.Linear(4 * number_of_embedding_dimensions, number_of_embedding_dimensions),\n",
    "            \n",
    "            # Occurs right before the merging back into the residual pathway\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1480a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Normalization\n",
    "# Normalizes the rows of every example individually\n",
    "# It used to be that the addition and normalization portions of transformers were carried out after\n",
    "# transformation, but typically now the normalization step is taken before said transformations\n",
    "class BatchNorm:\n",
    "    def __init__(self, dim, eps = 1e-5, momentum = 0.1):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Calculating the forward pass\n",
    "        \n",
    "         # Batch mean\n",
    "        x_mean = x.mean(1, keepdim = True)\n",
    "        \n",
    "         # Batch variance\n",
    "        x_variance = x.var(1, keepdim = True)\n",
    "        \n",
    "         # Normalizing to unit variance\n",
    "        x_hat = (x - x_mean) / torch.sqrt(x_variance + self.eps)\n",
    "        \n",
    "        self.out = self.gamma * x_hat + self.beta\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f445b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secondary Revised Bigram Language Model\n",
    "# Use of multi head attention greatly reduces the final trained model's overall loss\n",
    "class BigramLanguageModel3(nn.Module):\n",
    "    def __init__(self, vocabulary_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocabulary_size, number_of_embedding_dimensions)\n",
    "        self.postion_embedding_table == nn.Embedding(block_size, number_of_embedding_dimensions)\n",
    "        \n",
    "        # 4 Heads of 8 dimensional self attention that when concatenated results in a total of 32, the\n",
    "        # number of embedded dimensions, similar to group convolution\n",
    "        # 4 communications channels acting in parallel\n",
    "        self.self_attention_heads = MultiHeadAttention(4, number_of_embedding_dimensions // 4)\n",
    "        \n",
    "        # Enables the tokens to better contextualize and learn from the other tokens they are observing\n",
    "        # during the learning process\n",
    "        self.feed_forward = FeedForward(number_of_embedding_dimensions)\n",
    "        \n",
    "        self.language_modeling_head = nn.Linear(number_of_embedding_dimensions, vocabulary_size)\n",
    "        \n",
    "    def forward(self, index, targets = None):\n",
    "        batch, time = index.shape\n",
    "        token_embeddings = self.token_embedding_table(index)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(time, device = device))\n",
    "        token_id_position_embeddings = token_embeddings + position_embeddings\n",
    "        token_id_position_embeddings = self.self_attention_heads(token_id_position_embeddings)\n",
    "        \n",
    "        # Self attention is the communication gathering data, the feed forward is thinking on said data\n",
    "        # for each token, further reduces loss during model training\n",
    "        # Shape of (batch, time, channels)\n",
    "        token_id_position_embeddings = self.feed_forward(token_id_position_embeddings)\n",
    "        \n",
    "        logits = self.language_modeling_head(token_id_position_embeddings)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        \n",
    "        else:\n",
    "            batch, time, channels = logits.shape\n",
    "            logits = logits.view(batch * time, channels)\n",
    "            targets = targets.view(batch * time)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            index_condition = index[:, -block_size:]\n",
    "            logits, loss = self(index_condition)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim = -1)\n",
    "            index_next = torch.multinomial(probs, num_samples = 1)\n",
    "            index = torch.cat((index, index_next), dim = 1)\n",
    "            \n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d34e0c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Block\n",
    "# Interspersing communication with computation\n",
    "# Same act a transformer carries out using blocks to communicate and compute\n",
    "class Block(nn.Module):\n",
    "    \n",
    "    # Communication followed by computation\n",
    "    def __init__(self, number_of_embedding_dimensions, number_of_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Should be a head size 8 so that 4 blocks result in an overall dimension of 32\n",
    "        head_size = number_of_embedding_dimensions // number_of_heads\n",
    "        \n",
    "        # Communication\n",
    "        self.self_attention = MultiHeadAttention(number_of_heads, head_size)\n",
    "        \n",
    "        # Computation\n",
    "        self.feed_forward = FeedForward(number_of_embedding_dimensions)\n",
    "        \n",
    "        # Mean and variance are taken over the number of embedding dimensions, in our case 32 numbers\n",
    "        # Acting as a per token transformation, normalizing the features and rendering them \n",
    "        # unit mean and unit gautian at initialization, though this may change as the gamma and beta\n",
    "        # variables within during training might be optimized to change this\n",
    "        self.layer_normalization1 = nn.LayerNorm(number_of_embedding_dimensions)\n",
    "        self.layer_normalization2 = nn.LayerNorm(number_of_embedding_dimensions)\n",
    "    \n",
    "\n",
    "    # Skip & Residual Connections\n",
    "\n",
    "    # Transform data with a skip connection adding data from previous features\n",
    "    # Residual pathway is forked off, calculations and transformations carried out on fork, then recombined\n",
    "    # with the residual pathway via addition\n",
    "\n",
    "    # Addition distributes gradients equally to oth branches in terms of backpropagation\n",
    "\n",
    "    # Gradient superhighway from the supervision to the input, unimpeded, blocks kick in over time, having minimal\n",
    "    # impact at the start but becoming more important as training progresses, aiding in optimization by reducing\n",
    "    # the load\n",
    "    def forward(self, x):\n",
    "        # Forking off the residual path to carry out some communication then coming back\n",
    "        x = x + self.self_attention(self.layer_normalization1(x))\n",
    "        \n",
    "        # Forking off the residual path to carry out some computation then coming back\n",
    "        x = x + self.feed_forward(self.layer_normalization2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9c56b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tertiary Revised Bigram Language Model\n",
    "# Almost reaching the point of beginning to see overfitting due to the increasing size of the neural network\n",
    "class BigramLanguageModel4(nn.Module):\n",
    "    \n",
    "    # Decoder only transformer\n",
    "    def __init__(self, vocabulary_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocabulary_size, number_of_embedding_dimensions)\n",
    "        self.postion_embedding_table == nn.Embedding(block_size, number_of_embedding_dimensions)\n",
    "        \n",
    "        # At this point in time, the neural network is becoming relatively deep, resulting in \n",
    "        # optimization issues and a reduced level of improvement compared to past modifications\n",
    "        #self.blocks = nn.Sequential(\n",
    "        #    Block(number_of_embedding_dimensions, number_of_heads = 4), \n",
    "        #    Block(number_of_embedding_dimensions, number_of_heads = 4), \n",
    "        #    Block(number_of_embedding_dimensions, number_of_heads = 4), \n",
    "            \n",
    "            # Normalization layer right at the end of the transformer and right before the decoding layer\n",
    "        #    nn.LayerNorm(number_of_embedding_dimensions), \n",
    "        #)\n",
    "        \n",
    "        # For scaled version\n",
    "        self.blocks = nn.Sequential(*[Block(\n",
    "                                      number_of_embedding_dimensions, \n",
    "                                      number_of_heads = number_of_heads) \n",
    "                                      for _ in range(number_of_layers)])\n",
    "        \n",
    "        # Final layer normalization\n",
    "        self.layer_normalization = nn.LayerNorm(number_of_embedding_dimensions)\n",
    "        \n",
    "        self.language_modeling_head = nn.Linear(number_of_embedding_dimensions, vocabulary_size)\n",
    "        \n",
    "    def forward(self, index, targets = None):\n",
    "        batch, time = index.shape\n",
    "        token_embeddings = self.token_embedding_table(index)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(time, device = device))\n",
    "        token_id_position_embeddings = token_embeddings + position_embeddings\n",
    "        \n",
    "        # Shape of (batch, time, channels)\n",
    "        token_id_position_embeddings = self.blocks(token_id_position_embeddings) \n",
    "        \n",
    "        token_id_position_embeddings = self.layer_normalization(token_id_position_embeddings)\n",
    "        \n",
    "        logits = self.language_modeling_head(token_id_position_embeddings)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        \n",
    "        else:\n",
    "            batch, time, channels = logits.shape\n",
    "            logits = logits.view(batch * time, channels)\n",
    "            targets = targets.view(batch * time)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            index_condition = index[:, -block_size:]\n",
    "            logits, loss = self(index_condition)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim = -1)\n",
    "            index_next = torch.multinomial(probs, num_samples = 1)\n",
    "            index = torch.cat((index, index_next), dim = 1)\n",
    "            \n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4f966db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 65])\n",
      "tensor(4.7652, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocabulary_size)\n",
    "\n",
    "# Ensure that the model is also moved over to the appropirate device\n",
    "model = model.to(device)\n",
    "\n",
    "logits, loss = model(xb, yb)\n",
    "\n",
    "# This tensor contains the scores for every possible character which may come next\n",
    "print(logits.shape)\n",
    "\n",
    "# Loss is slightly higher than desired, meaning the guessing are wrong, but this is to be exepected\n",
    "# as the initial predictions are not yet diffuse and have a little bit of entropy\n",
    "# Loss of ~4.53 as opposed to ln(1 / 65) =  ~4.17\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "332b5787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "qvE33zoVPKeqAIldEzdXCJCoh'YE?JU'dP&uVu;ep\n",
      ";SwN&ubd-iND,.F!\n",
      "UlMH;fSUyru.TfSLiyek,pA D!pF$AfF'AVNKlepA\n"
     ]
    }
   ],
   "source": [
    "# (Batch = 1, Time = 1), 1 * 1 tensor holding a 0, used to kick off the generation and acting as the \n",
    "# first character of a sequence\n",
    "idx = torch.zeros((1, 1), dtype = torch.long)\n",
    "\n",
    "# As generate() works on the level of batches, the lone 0 tensor must be removed by indexing into the 0 throw\n",
    "# This ultimately generates a 1 dimensional row of indices that will then be converted into a simple Python list\n",
    "# This list from a tensor can then be decoded to covert the integers to characters\n",
    "print(decoder1(model.generate(idx, max_new_tokens = 100)[0].tolist()))\n",
    "\n",
    "# Result is garbage as the model is not yet trained and so outputs something completely random\n",
    "# Example Output:\n",
    "#qvE33zoVPKeqAIldEzdXCJCoh'YE?JU'dP&uVu;ep\n",
    "#;SwN&ubd-iND,.F!\n",
    "#UlMH;fSUyru.TfSLiyek,pA D!pF$AfF'AVNKlepA\n",
    "\n",
    "# As is, the model only looks at the immediately preceding character to make a prediction, and all other\n",
    "# characters in the context's history are ignored from the perspective of a bigram model, this will change\n",
    "# further on to avoid by so inefficient for no reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c9eb79e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a PyTorch optimizer\n",
    "# Learning rate is extremly low compared to most models due to how small the actual amount of data\n",
    "# is made available, allowing for such values\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b8f2d397",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4083828926086426\n"
     ]
    }
   ],
   "source": [
    "# Basic training loop\n",
    "batch_size = 32\n",
    "\n",
    "for steps in range(100000):\n",
    "    \n",
    "    # Sampling a batch of data\n",
    "    xb, yb = get_batch('training')\n",
    "    \n",
    "    # Evaluation the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    \n",
    "    # Zeroing out all gradients from the previous step\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    \n",
    "    # Retrieving the gradients for all of the current step's parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Utilizing the newly retrieved gradients to update said parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f4173c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I touarrtan\n",
      "SCave t, wize abe inrayetro e d or's knchen mmanthearomefonther y din:\n",
      "We have m RIfathe\n"
     ]
    }
   ],
   "source": [
    "# Results are hardly perfect, but seem to be making progress\n",
    "print(decoder1(model.generate(idx, max_new_tokens = 100)[0].tolist()))\n",
    "\n",
    "# Example Output:\n",
    "#I touarrtan\n",
    "#SCave t, wize abe inrayetro e d or's knchen mmanthearomefonther y din:\n",
    "#We have m RIfathe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ab01aaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# PyTorch does not find a GPU on Mac M1 Max, but already have something installed\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "473251e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averages out the loss over multiple batches\n",
    "# Avoids the noisy version of simply printing out of the loss directly from within the training loop\n",
    "# for the generator\n",
    "\n",
    "# Context manager indicating to PyTorch that everything occuring within this function will not be called\n",
    "# backward on for backpropagation,  making it far more efficient memory wise be not having to store all \n",
    "# those intermediate variables\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    \n",
    "    # Sets the model to the evaluation phase\n",
    "    # Important to take this into account, as neural network model layers have varying behavior between\n",
    "    # training and evaluation phases\n",
    "    model.eval()\n",
    "    \n",
    "    for split in ['training', 'validation']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        \n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        \n",
    "        out[split] = losses.mean()\n",
    "    \n",
    "    # Sets the model to the training phase\n",
    "    model.train()\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "70450973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Training Loss = 4.6858 | Validation Loss = 4.7224\n",
      "Step 300: Training Loss = 4.6879 | Validation Loss = 4.7171\n",
      "Step 600: Training Loss = 4.6893 | Validation Loss = 4.7289\n",
      "Step 900: Training Loss = 4.6978 | Validation Loss = 4.7212\n",
      "Step 1200: Training Loss = 4.6895 | Validation Loss = 4.7189\n",
      "Step 1500: Training Loss = 4.6937 | Validation Loss = 4.7136\n",
      "Step 1800: Training Loss = 4.6944 | Validation Loss = 4.7216\n",
      "Step 2100: Training Loss = 4.6979 | Validation Loss = 4.7175\n",
      "Step 2400: Training Loss = 4.6936 | Validation Loss = 4.7160\n",
      "Step 2700: Training Loss = 4.6854 | Validation Loss = 4.7240\n"
     ]
    }
   ],
   "source": [
    "# Proper training loop\n",
    "for iter in range(max_iters):\n",
    "    \n",
    "    # Evaluating losses on both the training and validation data sets every so often\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step {iter}: Training Loss = {losses['training']:.4f} | Validation Loss = {losses['validation']:.4f}\")\n",
    "    \n",
    "    # Sampling a batch of data\n",
    "    xb, yb = get_batch('training')\n",
    "    \n",
    "    # Evaluating the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "190f7176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mVS&o!qd-CvZA.sE\n",
      "akcU\n",
      "lWX.kv-UC:cugYeaNuc,tnyeHKf;W,s bHH!qdvKr&e;WtfL&qzENuLBNLizNx HsA,mg:clUkz$XuyBXOrNSxJ\n",
      "G!bNn!qSqh!quXSxGuLu!qsbcS\n",
      "usJTYFFxrggRWMKfVvWQnZ?Lj,S:&fW&r?EDXG-iP.rhFKlGypK'l$yMX&,fGggqu\n",
      ";e,XGoHkPXO-i.rXVX.LoJm:c,?:VrNqItyZJSSxdMBnnKrUMzZDgg YVp3AVS:cpZTUJSwsZKjYF-ulh:TEsZVtq,.auZAjwn:EDBs'hqBAvIL!Xu3gLCXOehoo:cVI vvH.krPKxTQK$I-?P.ajkfLl'BNhwnPvtZmHP'OAV.sZojm$EA.ZqvI'V&3hul!qfVGowuVmC,Vv,SxZybrBe:Th--!XwoRrbOrd,;VXAe:YAG\n",
      "GH!LRoLVTz quZZFxp?rGusTEm3YTzYQgggBNPKmsEmHKFRpvlQJxz,mX\n"
     ]
    }
   ],
   "source": [
    "# Generating from the model\n",
    "context = torch.zeros((1, 1), dtype = torch.long, device = device)\n",
    "print(decoder1(model.generate(context, max_new_tokens = 500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "004b35f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mathematical trick in self attention\n",
    "# Desire for the tokens from the time dimension to speak to all those tokens which came before them, but not\n",
    "# to those which came after\n",
    "\n",
    "# Calculating the average of all the vectors from the previous and current token is one method to enable\n",
    "# interaction, but it is extremely weak, as a lot of information is lost such as positioning\n",
    "\n",
    "# Toy Example\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8 ,2 # Batch, Time, CHannels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7d96c6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialized at zero\n",
    "x_bag_of_words = torch.zeros((B, T, C))\n",
    "\n",
    "# Iterating over every batch dimension independently\n",
    "# For loops are inneficient, better to carry out this process via matrix multiplication\n",
    "for b in range(B):\n",
    "    \n",
    "    # Iterating over time\n",
    "    for t in range(T):\n",
    "        \n",
    "        # Previous tokens include everything within the batch dimension up to the time\n",
    "        # The chunk of tokens previous to the current token for the given sequence\n",
    "        xprev = x[b, :t + 1] # Shape (t elements in past, C)\n",
    "        \n",
    "        # Averaging out the time dimension, resulting in a 1D (C) vector\n",
    "        x_bag_of_words[b, t] = torch.mean(xprev, 0)\n",
    "        \n",
    "# Every new row is now the average of the previous rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "fe9f2dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "850ae6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bag_of_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a6f966c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[8., 9.],\n",
      "        [2., 7.],\n",
      "        [3., 9.]])\n",
      "tensor([[8.0000, 9.0000],\n",
      "        [5.0000, 8.0000],\n",
      "        [4.3333, 8.3333]])\n"
     ]
    }
   ],
   "source": [
    "# Can quickly accomplish this via matrix multiplication\n",
    "a1 = torch.ones(3, 3)\n",
    "a2 = torch.tril(a1)\n",
    "\n",
    "# Generating a matrix with multiplication coefficients that when used in matrix multiplication return\n",
    "# a final matrix of progressing incremental averages\n",
    "a2 = a2 / torch.sum(a2, 1, keepdim = True)\n",
    "\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a2 @ b\n",
    "\n",
    "print(a1)\n",
    "print(a2)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2a26c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tril(torch.ones(T, T))\n",
    "weights = weights / weights.sum(1, keepdim = True)\n",
    "\n",
    "# Batch matrix multiplication will be done in parallel and individually, weighted aggregation\n",
    "# Weight matrix @ data matrix\n",
    "# (B, T, T) @ (B, T, C) = (B, T, C)\n",
    "x_bag_of_words = weights @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e84129ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "36dea184",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4313d435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Introducing softmax\n",
    "\n",
    "# Starting with a tensor of shape (T * T) of ones\n",
    "tril = torch.ones(T, T)\n",
    "\n",
    "# Applying the tril function to the tensor to obtain a diagonal fill of ones\n",
    "tril = torch.tril(tril)\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "df589209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting with a tensor of zeros of shape (T * T) for the weights tensor\n",
    "# How many tokens are desired for aggregation and averaging so as to transfer information\n",
    "# In usage, some tokens will find others more or less interesting, affinity is affected and will\n",
    "# have an impact on the final output\n",
    "weights = torch.zeros((T, T))\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "65945467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For all elements within the tril tensor = 0, fill in the appropriate \n",
    "# elements within the weights tensor with '-inf'\n",
    "# Tokens on '-inf' will not be aggregated, tokens from the past cannot communicate with those of the future\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7632a631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applies a softmax aggregation across every single row of the weights tensor'\n",
    "# Exponentiates every element in a row, so 0 becomes 1 and -inf becomes 0, then these are averaged\n",
    "weights = F.softmax(weights, dim = -1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "205e724a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow3 = weights @ x\n",
    "torch.allclose(x_bag_of_words, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d7bbc3b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Self Attention\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# A 4 by 8 arrangement of tokens, with each token having information be 32 dimensional\n",
    "batch, time, channels = 4, 8, 32\n",
    "x = torch.randn(batch, time, channels)\n",
    "\n",
    "# Carries out simple average of all past tokens and the current token\n",
    "tril = torch.tril(torch.ones(time, time))\n",
    "\n",
    "# Affinities are being initialized at 0, losing out on certain tokens being more or less interested in others\n",
    "# An example would be a consonate not knowing if another consonate or vowel came just before it to be able\n",
    "# to later on make a better prediction, which is lost in this method\n",
    "weights = torch.zeros((time, time))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim = -1)\n",
    "output = weights @ x\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a24e34ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attention is a method of communication, with vectors of information lying around in space, tokens / nodes\n",
    "# aggregating the information being pointed towards them by other tokens\n",
    "# Compared to CNNs, there is no concept of space, orientation, positioning, etc., so in order to have said\n",
    "# tokens take this into consideration, the information must in some way be included in the vectors of information\n",
    "\n",
    "# Example of character based word creation for the word 'home': Tokens pointing with context to space and position\n",
    "# h -> o\n",
    "# h and o -> m\n",
    "# h and o and m -> e\n",
    "\n",
    "# Tokens want information to flow to them that they find most interesting or useful\n",
    "# Gather information from the past in a data dependent way\n",
    "\n",
    "# Every token will emit 2 vectors, a query and a key\n",
    "# Query vector is what is the token looking for\n",
    "# Key vector is what the token contains\n",
    "\n",
    "# The affinities formed between tokens now form from the dot product \n",
    "# of a query upon all the keys of the other tokens, become weights\n",
    "\n",
    "# If a key and query align well, than more will be learned between the \n",
    "# 2 involved tokens than any others in the sequence\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "batch, time, channels = 4, 8, 32\n",
    "x = torch.randn(batch, time, channels)\n",
    "\n",
    "# Single head performing self attention\n",
    "head_size = 16 # Hyperparameter\n",
    "\n",
    "# Bias set to false to avoid matrix mutliplication with fixed weights\n",
    "key = nn.Linear(channels, head_size, bias = False)\n",
    "query = nn.Linear(channels, head_size, bias = False)\n",
    "value = nn.Linear(channels, head_size, bias = False)\n",
    "\n",
    "# When linear key and query torches are forwarded atop of the 'x' tensor of tokens, all tokens in all positions\n",
    "# of the batch by time arrangement will independently, in parallel, and without communicating create their \n",
    "# own unique key and query\n",
    "k = key(x) # Shape of (batch, time, head_size)\n",
    "q = query(x) # Shape of (batch, time, head_size)\n",
    "\n",
    "# All queries must now dot product with all the keys to produce affinity\n",
    "# Avoid the batch dimension\n",
    "# For every row of a batch, there will be a time squared matrix providing the affinities as weights\n",
    "weights = q @ k.transpose(-2, -1) # (batch, time, head_size) @ (batch, head_size, time) = (batch, time, time)\n",
    "\n",
    "tril = torch.tril(torch.ones(time, time))\n",
    "\n",
    "\n",
    "# Encoder Block\n",
    "# Masking used to hide tokens in the 'future' from those in the past to avoid affinity which shouldn't be possible\n",
    "\n",
    "# In cases of sentiment analysis, for example, you might want future words to talk to past words so as to be\n",
    "# able to understand concepts such as sarcasm or what not, as words in the future of a piece of text may invalidate\n",
    "# or spin the meaning of words in the past despite what their typical usage might imply, the 'BUT'\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "\n",
    "# Normalizing values to inform the current token how much information to aggregate from those in the past\n",
    "weights = F.softmax(weights, dim = -1)\n",
    "\n",
    "# 'v' is the vector being aggregated rather than the raw 'x'\n",
    "# 'x' is information private to only an individual token, the key being what it has, the query being what\n",
    "# it is interested in, and the value being what another token might want to know if they are interest in it\n",
    "v = value(x)\n",
    "output = weights @ v\n",
    "#output = weights @ x\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0f77c4ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
       "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
       "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
       "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
       "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
       "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
       "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weights are no longer constants, they are unique as each batch element have differing tokens\n",
    "# Now data dependent\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897fb54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self attention is when the source of the token's keys, queries and values all originate from the same source\n",
    "\n",
    "# Cross attention is where there is a seperate source of tokens off to the side where information is pulled from\n",
    "# outside of the current data and only the queries are local, the keys and values coming from an external source\n",
    "# and sometimes encoder blocks based on a preconditioned context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2dd9578e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0001)\n",
      "tensor(0.9428)\n",
      "tensor(1.1663)\n"
     ]
    }
   ],
   "source": [
    "# To properly carry out the theoretical to the practical, technically a scaled attention should be applied, \n",
    "# however since weights are then fed into softmax in this demonstration, the weights should be fairly diffuse\n",
    "# going in, as if there are opposite extremes of very positive and negative values, than softmax will force\n",
    "# a convergence towards 1 hot vectors, essentially ignoring all other tokens or nodes in terms of aggregating\n",
    "# information\n",
    "\n",
    "# Scaling used to control variance at initialization\n",
    "batch, time, head_size = 4, 8, 16\n",
    "\n",
    "k = torch.randn(batch, time, head_size)\n",
    "q = torch.randn(batch, time, head_size)\n",
    "weights = q @ k.transpose(-2, -1) * head_size ** -0.5\n",
    "\n",
    "print(k.var())\n",
    "print(q.var())\n",
    "print(weights.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e928eb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "# Diffuse weights\n",
    "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim = -1))\n",
    "\n",
    "# One hot weights, sharpened towards the originally highest valued weight via an example multiplication\n",
    "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 8, dim = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c82838d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100])\n"
     ]
    }
   ],
   "source": [
    "# Layer Normalization\n",
    "torch.manual_seed(1337)\n",
    "module = BatchNorm(100)\n",
    "x = torch.randn(32, 100)\n",
    "x = module(x)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "337afed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean and standard deviation of one feature across all batch inputs\n",
    "x[:, 0].mean(), x[:, 0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b034627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-3.5763e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean and standard deviation of a single input from the batch and of its features\n",
    "x[0, :].mean(), x[0, :].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee9faa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
